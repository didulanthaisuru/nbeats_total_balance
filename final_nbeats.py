#!/usr/bin/env python3
"""
NBEATS Balance Prediction - Complete Production Pipeline
======================================================

This script provides a complete end-to-end balance prediction system using NBEATSx
with comprehensive null handling, SQLite-based hyperparameter optimization, and
detailed visualizations.

Author: Balance Prediction Team
Date: July 2025
Version: 1.0 (Production Ready)
"""

# =============================================================================
# BLOCK 1: IMPORTS AND DEPENDENCIES
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
import os
import time
import warnings
from pathlib import Path
from datetime import datetime, timedelta
import logging

# Machine Learning and Forecasting
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATSx
from neuralforecast.losses.pytorch import DistributionLoss
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy import stats

# Hyperparameter Optimization
import optuna
from optuna.storages import RDBStorage
from optuna.trial import Trial

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

print("ğŸš€ NBEATS BALANCE PREDICTION SYSTEM")
print("="*80)
print("âœ… All dependencies imported successfully")
print("âœ… Production-ready pipeline initialized")
print("="*80)

# =============================================================================
# BLOCK 2: DATA LOADING AND INITIAL INSPECTION
# =============================================================================

def load_and_inspect_data(file_path="/kaggle/input/shiharadataset/featured_shihara.xlsx"):
    """
    Load balance data and perform initial inspection.
    
    Args:
        file_path (str): Path to the Excel file containing balance data
        
    Returns:
        pd.DataFrame: Loaded dataframe
    """
    
    print("\nğŸ“‚ LOADING AND INSPECTING DATA")
    print("="*60)
    
    try:
        # Load data
        df = pd.read_excel(file_path)
        logger.info(f"Data loaded successfully from {file_path}")
        
        # Basic information
        print(f"ğŸ“Š Dataset shape: {df.shape}")
        print(f"ğŸ“… Columns: {list(df.columns)}")
        
        # Data types
        print(f"\nğŸ“ˆ Data types:")
        for col, dtype in df.dtypes.items():
            print(f"   {col}: {dtype}")
        
        # Date range analysis
        if 'Date' in df.columns:
            print(f"\nğŸ“… Date range: {df['Date'].min()} to {df['Date'].max()}")
            print(f"ğŸ“… Total days: {(df['Date'].max() - df['Date'].min()).days}")
        
        # Target variable analysis
        if 'Normalized_Balance' in df.columns:
            target = df['Normalized_Balance']
            print(f"\nğŸ’° Balance statistics:")
            print(f"   Min: {target.min():.4f}")
            print(f"   Max: {target.max():.4f}")
            print(f"   Mean: {target.mean():.4f}")
            print(f"   Std: {target.std():.4f}")
        
        print(f"\nâœ… Data loading complete: {df.shape[0]} rows, {df.shape[1]} columns")
        return df
        
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        print(f"âŒ Error loading data: {str(e)}")
        return None

# =============================================================================
# BLOCK 3: COMPREHENSIVE NULL VALUE HANDLING
# =============================================================================

def detect_null_patterns(df, verbose=True):
    """
    Analyze null value patterns in the dataset.
    
    Args:
        df (pd.DataFrame): Input dataframe
        verbose (bool): Print detailed analysis
        
    Returns:
        dict: Null pattern analysis results
    """
    
    if verbose:
        print("\nğŸ” NULL PATTERN ANALYSIS")
        print("="*50)
    
    analysis = {
        'null_by_column': df.isnull().sum(),
        'total_nulls': df.isnull().sum().sum(),
        'null_percentage': (df.isnull().sum().sum() / df.size) * 100,
        'problematic_columns': []
    }
    
    # Check each column
    for col in df.columns:
        null_count = df[col].isnull().sum()
        null_pct = (null_count / len(df)) * 100
        
        if null_count > 0:
            analysis['problematic_columns'].append({
                'column': col,
                'null_count': null_count,
                'null_percentage': null_pct
            })
            
            if verbose:
                print(f"âš ï¸  {col}: {null_count} nulls ({null_pct:.2f}%)")
    
    if verbose:
        if analysis['total_nulls'] == 0:
            print("âœ… No null values found in dataset")
        else:
            print(f"\nğŸ“Š Total nulls: {analysis['total_nulls']}")
            print(f"ğŸ“Š Overall null percentage: {analysis['null_percentage']:.2f}%")
    
    return analysis

def handle_null_values(df, method='auto', verbose=True):
    """
    Comprehensive null value handling for balance prediction dataset.
    
    Args:
        df (pd.DataFrame): Input dataframe
        method (str): Handling method ('auto', 'conservative', 'aggressive')
        verbose (bool): Print detailed information
        
    Returns:
        pd.DataFrame: Cleaned dataframe
    """
    
    if verbose:
        print("\nğŸ› ï¸ COMPREHENSIVE NULL VALUE HANDLING")
        print("="*60)
    
    df_cleaned = df.copy()
    original_nulls = df_cleaned.isnull().sum().sum()
    
    # Column categorization
    date_columns = [col for col in df_cleaned.columns if 'date' in col.lower()]
    target_columns = [col for col in df_cleaned.columns if 'normalized_balance' in col.lower() or col.lower() == 'y']
    lag_features = [col for col in df_cleaned.columns if 'ago' in col.lower() or 'lag' in col.lower()]
    rolling_features = [col for col in df_cleaned.columns if 'rolling' in col.lower() or ('mean' in col.lower() and 'd' in col.lower())]
    categorical_features = [col for col in df_cleaned.columns if df_cleaned[col].dtype == 'object' and col not in date_columns]
    
    if verbose:
        print(f"ğŸ“‹ Column categorization:")
        print(f"   ğŸ“… Date columns: {date_columns}")
        print(f"   ğŸ¯ Target columns: {target_columns}")
        print(f"   â®ï¸ Lag features: {lag_features}")
        print(f"   ğŸ“Š Rolling features: {rolling_features}")
        print(f"   ğŸ·ï¸ Categorical features: {categorical_features}")
    
    # Handle each category
    
    # 1. Date columns - Remove rows with null dates
    for col in date_columns:
        if df_cleaned[col].isnull().any():
            before_count = len(df_cleaned)
            df_cleaned = df_cleaned.dropna(subset=[col])
            removed = before_count - len(df_cleaned)
            if verbose:
                print(f"ğŸ“… {col}: Removed {removed} rows with null dates")
    
    # 2. Target columns - Linear interpolation + statistical fill
    for col in target_columns:
        if df_cleaned[col].isnull().any():
            null_count = df_cleaned[col].isnull().sum()
            if verbose:
                print(f"ğŸ¯ {col}: Handling {null_count} nulls with interpolation")
            
            # Linear interpolation
            df_cleaned[col] = df_cleaned[col].interpolate(method='linear')
            # Fill remaining with forward/backward fill
            df_cleaned[col] = df_cleaned[col].fillna(method='ffill').fillna(method='bfill')
            # Final fallback to mean
            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())
    
    # 3. Lag features - Forward fill (logical for lags)
    for col in lag_features:
        if df_cleaned[col].isnull().any():
            null_count = df_cleaned[col].isnull().sum()
            if verbose:
                print(f"â®ï¸ {col}: Handling {null_count} nulls with forward fill")
            
            df_cleaned[col] = df_cleaned[col].fillna(method='ffill')
            df_cleaned[col] = df_cleaned[col].fillna(method='bfill')
            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())
    
    # 4. Rolling features - Interpolation + median
    for col in rolling_features:
        if df_cleaned[col].isnull().any():
            null_count = df_cleaned[col].isnull().sum()
            if verbose:
                print(f"ğŸ“Š {col}: Handling {null_count} nulls with interpolation + median")
            
            df_cleaned[col] = df_cleaned[col].interpolate(method='linear')
            if 'std' in col.lower():
                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())
            else:
                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())
    
    # 5. Categorical features - Mode fill
    for col in categorical_features:
        if df_cleaned[col].isnull().any():
            null_count = df_cleaned[col].isnull().sum()
            if verbose:
                print(f"ğŸ·ï¸ {col}: Handling {null_count} nulls with mode")
            
            mode_value = df_cleaned[col].mode()
            if len(mode_value) > 0:
                df_cleaned[col] = df_cleaned[col].fillna(mode_value[0])
            else:
                df_cleaned[col] = df_cleaned[col].fillna('unknown')
    
    # 6. Remaining numeric columns - Interpolation + mean
    remaining_cols = [col for col in df_cleaned.columns 
                     if col not in date_columns + target_columns + lag_features + rolling_features + categorical_features
                     and df_cleaned[col].dtype in ['int64', 'float64']]
    
    for col in remaining_cols:
        if df_cleaned[col].isnull().any():
            null_count = df_cleaned[col].isnull().sum()
            if verbose:
                print(f"ğŸ”¢ {col}: Handling {null_count} nulls with interpolation + mean")
            
            df_cleaned[col] = df_cleaned[col].interpolate(method='linear')
            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())
    
    # Final validation
    final_nulls = df_cleaned.isnull().sum().sum()
    
    if verbose:
        print(f"\nâœ… NULL HANDLING COMPLETE")
        print(f"ğŸ“Š Original nulls: {original_nulls}")
        print(f"ğŸ“Š Final nulls: {final_nulls}")
        print(f"ğŸ“Š Rows preserved: {len(df_cleaned)}/{len(df)} ({(len(df_cleaned)/len(df)*100):.1f}%)")
        
        if final_nulls > 0:
            print(f"âš ï¸ Warning: {final_nulls} nulls remain")
        else:
            print("ğŸ‰ All nulls successfully handled!")
    
    return df_cleaned

# =============================================================================
# BLOCK 4: DATA SPLITTING AND PREPARATION
# =============================================================================

def prepare_train_test_split(df, test_days=30, validation_days=15, verbose=True):
    """
    Split data into train, validation, and test sets for time series.
    
    Args:
        df (pd.DataFrame): Cleaned input dataframe
        test_days (int): Number of days for test set
        validation_days (int): Number of days for validation set
        verbose (bool): Print split information
        
    Returns:
        tuple: (train_df, val_df, test_df, split_info)
    """
    
    if verbose:
        print("\nğŸ“Š PREPARING TRAIN/VALIDATION/TEST SPLIT")
        print("="*60)
    
    # Ensure data is sorted by date
    df_sorted = df.sort_values('Date').reset_index(drop=True)
    
    # Calculate split indices
    total_days = len(df_sorted)
    test_start = total_days - test_days
    val_start = test_start - validation_days
    
    # Create splits
    train_df = df_sorted.iloc[:val_start].copy()
    val_df = df_sorted.iloc[val_start:test_start].copy()
    test_df = df_sorted.iloc[test_start:].copy()
    
    split_info = {
        'total_days': total_days,
        'train_days': len(train_df),
        'val_days': len(val_df),
        'test_days': len(test_df),
        'train_date_range': (train_df['Date'].min(), train_df['Date'].max()),
        'val_date_range': (val_df['Date'].min(), val_df['Date'].max()),
        'test_date_range': (test_df['Date'].min(), test_df['Date'].max())
    }
    
    if verbose:
        print(f"ğŸ“ˆ Training set: {len(train_df)} days ({len(train_df)/total_days*100:.1f}%)")
        print(f"   ğŸ“… Date range: {split_info['train_date_range'][0]} to {split_info['train_date_range'][1]}")
        
        print(f"ğŸ“Š Validation set: {len(val_df)} days ({len(val_df)/total_days*100:.1f}%)")
        print(f"   ğŸ“… Date range: {split_info['val_date_range'][0]} to {split_info['val_date_range'][1]}")
        
        print(f"ğŸ§ª Test set: {len(test_df)} days ({len(test_df)/total_days*100:.1f}%)")
        print(f"   ğŸ“… Date range: {split_info['test_date_range'][0]} to {split_info['test_date_range'][1]}")
        
        print(f"\nâœ… Data split complete: {len(train_df)} + {len(val_df)} + {len(test_df)} = {total_days} days")
    
    return train_df, val_df, test_df, split_info

def prepare_neuralforecast_data(train_df, val_df, test_df, verbose=True):
    """
    Convert dataframes to NeuralForecast format.
    
    Args:
        train_df, val_df, test_df: Split dataframes
        verbose (bool): Print conversion info
        
    Returns:
        tuple: (train_data, val_data, test_data, feature_lists)
    """
    
    if verbose:
        print("\nğŸ”„ CONVERTING TO NEURALFORECAST FORMAT")
        print("="*60)
    
    # Convert to NeuralForecast format
    def convert_to_nf_format(df):
        nf_df = df.copy()
        nf_df['unique_id'] = 'balance'
        nf_df = nf_df.rename(columns={'Date': 'ds', 'Normalized_Balance': 'y'})
        return nf_df
    
    train_data = convert_to_nf_format(train_df)
    val_data = convert_to_nf_format(val_df)
    test_data = convert_to_nf_format(test_df)
    
    # Identify available features
    potential_future = ['dayofweek_sin', 'dayofweek_cos']
    potential_historical = [
        'balance_changed', 'balance_1d_ago', 'balance_7d_ago', 
        'balance_30d_ago', 'rolling_mean_30d', 'rolling_std_30d'
    ]
    
    future_features = [feat for feat in potential_future if feat in train_data.columns]
    historical_features = [feat for feat in potential_historical if feat in train_data.columns]
    
    feature_lists = {
        'future_features': future_features,
        'historical_features': historical_features
    }
    
    if verbose:
        print(f"ğŸ”§ Future features: {future_features}")
        print(f"ğŸ”§ Historical features: {historical_features}")
        print(f"âœ… NeuralForecast format conversion complete")
    
    return train_data, val_data, test_data, feature_lists

# =============================================================================
# BLOCK 5: SQLITE-BASED HYPERPARAMETER OPTIMIZATION
# =============================================================================

def setup_sqlite_storage(storage_path="./optuna_balance_study.db", study_name="nbeats_balance_optimization"):
    """
    Setup SQLite storage for Optuna hyperparameter optimization.
    
    Args:
        storage_path (str): Path to SQLite database
        study_name (str): Name of the optimization study
        
    Returns:
        tuple: (storage, study)
    """
    
    print("\nğŸ’¾ SETTING UP SQLITE STORAGE FOR HYPERPARAMETER OPTIMIZATION")
    print("="*70)
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(os.path.abspath(storage_path)), exist_ok=True)
    
    # Configure SQLite storage
    storage_url = f"sqlite:///{storage_path}"
    engine_kwargs = {
        'connect_args': {
            'timeout': 300,  # 5 minutes timeout
            'check_same_thread': False,
            'isolation_level': None  # Autocommit mode
        },
        'pool_pre_ping': True,
        'pool_recycle': 1800  # 30 minutes
    }
    
    try:
        # Create storage
        storage = RDBStorage(
            url=storage_url,
            engine_kwargs=engine_kwargs,
            heartbeat_interval=60,
            grace_period=120
        )
        
        # Create or load study
        study = optuna.create_study(
            study_name=study_name,
            storage=storage,
            direction='minimize',
            load_if_exists=True,
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        print(f"âœ… SQLite storage created: {storage_path}")
        print(f"âœ… Study '{study_name}' initialized")
        print(f"ğŸ“Š Previous trials: {len(study.trials)}")
        
        return storage, study
        
    except Exception as e:
        logger.error(f"SQLite storage setup failed: {str(e)}")
        print(f"âŒ SQLite setup failed: {str(e)}")
        print("ğŸ”„ Falling back to in-memory storage...")
        
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        return None, study

def hyperparameter_optimization(train_data, val_data, feature_lists, 
                               horizon=30, n_trials=20, 
                               storage_path="./optuna_balance_study.db",
                               study_name="nbeats_balance_optimization"):
    """
    Perform hyperparameter optimization using Optuna with SQLite storage.
    
    Args:
        train_data, val_data: Training and validation datasets
        feature_lists (dict): Future and historical feature lists
        horizon (int): Forecast horizon
        n_trials (int): Number of optimization trials
        storage_path (str): SQLite database path
        study_name (str): Study name
        
    Returns:
        tuple: (best_parameters, best_model, optimization_results)
    """
    
    print("\nğŸ¯ HYPERPARAMETER OPTIMIZATION WITH SQLITE STORAGE")
    print("="*70)
    
    # Setup storage
    storage, study = setup_sqlite_storage(storage_path, study_name)
    
    # Feature lists
    future_features = feature_lists['future_features']
    historical_features = feature_lists['historical_features']
    
    # Store best model globally
    best_model = None
    best_rmse = float('inf')
    
    def create_future_features(df, horizon):
        """Create future features for prediction using exact validation combinations"""
        print(f"ğŸ”§ Creating future features for {horizon} periods...")
        print(f"ğŸ”§ Input dataframe shape: {df.shape}")
        print(f"ğŸ”§ Input columns: {list(df.columns)}")

        # For validation, use the exact ds and unique_id combinations from val_data
        # Assume horizon == len(val_data) and val_data is available in closure
        # If not, fallback to previous method
        try:
            # Use val_data from closure
            val_ids = val_data['unique_id'].values
            val_dates = val_data['ds'].values
            future_df = pd.DataFrame({'unique_id': val_ids, 'ds': val_dates})
            print(f"ğŸ”§ Using validation unique_id/ds combinations: {future_df.shape}")
        except Exception as e:
            print(f"âš ï¸ Could not use val_data for future_df: {e}, falling back to previous method")
            # Fallback: use previous manual method
            unique_ids = df['unique_id'].unique()
            last_dates = df.groupby('unique_id')['ds'].max()
            future_dfs = []
            for uid in unique_ids:
                last_date = last_dates[uid]
                future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon, freq='D')
                uid_future = pd.DataFrame({'unique_id': uid, 'ds': future_dates})
                future_dfs.append(uid_future)
            future_df = pd.concat(future_dfs, ignore_index=True)

        # Add time-based features
        future_df['dayofweek_sin'] = np.sin(2 * np.pi * pd.to_datetime(future_df['ds']).dt.dayofweek / 7)
        future_df['dayofweek_cos'] = np.cos(2 * np.pi * pd.to_datetime(future_df['ds']).dt.dayofweek / 7)
        print(f"ğŸ”§ Added time-based features")

        # Add ALL other columns from training data (except 'y')
        train_columns = set(df.columns) - {'y'}
        future_columns = set(future_df.columns)
        missing_columns = train_columns - future_columns

        print(f"ğŸ”§ Training columns: {len(train_columns)} - {sorted(train_columns)}")
        print(f"ğŸ”§ Future columns: {len(future_columns)} - {sorted(future_columns)}")
        print(f"ğŸ”§ Missing columns: {len(missing_columns)} - {sorted(missing_columns)}")

        # Add all missing columns with appropriate values
        for col in missing_columns:
            print(f"ğŸ”§ Processing missing column: {col}")
            if col in df.columns:
                last_values = df.groupby('unique_id')[col].last()
                future_df[col] = future_df['unique_id'].map(last_values)
                if future_df[col].isnull().any():
                    if df[col].dtype in ['float64', 'int64']:
                        default_val = df[col].median() if not df[col].isnull().all() else 0
                        future_df[col] = future_df[col].fillna(default_val)
                        print(f"ğŸ”§ Filled NaNs in {col} with median: {default_val}")
                    else:
                        mode_val = df[col].mode()
                        default_val = mode_val[0] if len(mode_val) > 0 else 'unknown'
                        future_df[col] = future_df[col].fillna(default_val)
                        print(f"ğŸ”§ Filled NaNs in {col} with mode: {default_val}")
            else:
                default_val = 0 if pd.api.types.is_numeric_dtype(df.select_dtypes(include=[np.number]).columns) else 'unknown'
                future_df[col] = default_val
                print(f"ğŸ”§ Added new column {col} with default: {default_val}")

        # Ensure proper column order matching training data
        train_col_order = [col for col in df.columns if col != 'y']
        missing_cols = set(train_col_order) - set(future_df.columns)
        if missing_cols:
            print(f"âš ï¸ Still missing columns: {missing_cols}")
            for col in missing_cols:
                future_df[col] = 0
                print(f"ğŸ”§ Added missing column {col} with value 0")
        try:
            future_df = future_df[train_col_order]
            print(f"ğŸ”§ Column reordering successful - order matches training data")
        except KeyError as e:
            print(f"âš ï¸ Column reordering failed: {e}")
            available_cols = [col for col in train_col_order if col in future_df.columns]
            future_df = future_df[available_cols]
            print(f"ğŸ”§ Using available columns: {len(available_cols)}")

        # Final data type consistency check
        for col in future_df.columns:
            if col in df.columns:
                if df[col].dtype != future_df[col].dtype:
                    try:
                        future_df[col] = future_df[col].astype(df[col].dtype)
                        print(f"ğŸ”§ Converted {col} to {df[col].dtype}")
                    except:
                        print(f"âš ï¸ Could not convert {col} to {df[col].dtype}")

        # Handle any remaining nulls
        null_cols = future_df.columns[future_df.isnull().any()].tolist()
        if null_cols:
            print(f"ğŸ”§ Handling nulls in columns: {null_cols}")
            for col in null_cols:
                null_count = future_df[col].isnull().sum()
                if future_df[col].dtype in ['float64', 'int64']:
                    future_df[col] = future_df[col].fillna(0)
                else:
                    future_df[col] = future_df[col].fillna('unknown')
                print(f"ğŸ”§ Filled {null_count} nulls in {col}")

        # Final validation and reporting
        print(f"ğŸ”§ Final future dataframe: {future_df.shape}")
        print(f"ğŸ”§ Final columns: {list(future_df.columns)}")
        print(f"ğŸ”§ Training columns (no y): {[col for col in df.columns if col != 'y']}")

        train_features = set(df.columns) - {'y'}
        future_features = set(future_df.columns)
        features_match = train_features == future_features
        print(f"ğŸ”§ Features match perfectly: {features_match}")
        if not features_match:
            print(f"âš ï¸ Feature mismatch detected!")
            print(f"   Missing in future: {train_features - future_features}")
            print(f"   Extra in future: {future_features - train_features}")
        else:
            print(f"âœ… Perfect feature alignment achieved!")

        print(f"ğŸ”§ Final validation:")
        print(f"   Shape: {future_df.shape}")
        print(f"   Date range: {future_df['ds'].min()} to {future_df['ds'].max()}")
        print(f"   Unique IDs: {sorted(future_df['unique_id'].unique())}")
        print(f"   No nulls: {not future_df.isnull().any().any()}")
        print(f"   Actual rows: {len(future_df)}")

        return future_df
    
    def objective(trial: Trial) -> float:
        """Objective function for optimization"""
        nonlocal best_model, best_rmse
        
        trial_start = time.time()
        
        try:
            # Hyperparameter space
            input_size = trial.suggest_int('input_size', 100, 200)
            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
            max_steps = trial.suggest_int('max_steps', 500, 1500)
            batch_size = trial.suggest_int('batch_size', 16, 48)
            n_blocks = [trial.suggest_int(f'n_blocks_{i}', 2, 4) for i in range(3)]
            n_harmonics = trial.suggest_int('n_harmonics', 1, 3)
            n_polynomials = trial.suggest_int('n_polynomials', 1, 3)
            
            print(f"ğŸ”§ Trial {trial.number}: Testing parameters...")
            print(f"   Input size: {input_size}, LR: {learning_rate:.2e}, Steps: {max_steps}")
            
            # Create model
            model = NBEATSx(
                h=horizon,
                input_size=input_size,
                futr_exog_list=future_features,
                hist_exog_list=historical_features,
                random_seed=42,
                scaler_type='standard',
                learning_rate=learning_rate,
                max_steps=max_steps,
                batch_size=batch_size,
                stack_types=['identity', 'trend', 'seasonality'],
                n_blocks=n_blocks,
                n_harmonics=n_harmonics,
                n_polynomials=n_polynomials,
                loss=DistributionLoss(distribution='Normal', level=[80, 95])
            )
            
            # Create and train forecaster
            forecaster = NeuralForecast(models=[model], freq='D')
            print(f"ğŸ‹ï¸ Training model for trial {trial.number}...")
            forecaster.fit(df=train_data)
            
            # Generate predictions on validation set
            print(f"ğŸ”® Creating future features for validation...")
            future_df = create_future_features(train_data, len(val_data))
            
            print(f"ğŸ“Š Training data shape: {train_data.shape}")
            print(f"ğŸ“Š Future data shape: {future_df.shape}")
            print(f"ğŸ“Š Validation data shape: {val_data.shape}")
            
            # Additional validation before prediction
            print(f"ğŸ” Pre-prediction validation:")
            print(f"   Training unique_ids: {sorted(train_data['unique_id'].unique())}")
            print(f"   Future unique_ids: {sorted(future_df['unique_id'].unique())}")
            print(f"   Training date range: {train_data['ds'].min()} to {train_data['ds'].max()}")
            print(f"   Future date range: {future_df['ds'].min()} to {future_df['ds'].max()}")
            
            # Check for any obvious issues
            if future_df.shape[0] == 0:
                raise ValueError("Future dataframe is empty!")
            
            if len(future_df['unique_id'].unique()) == 0:
                raise ValueError("No unique_ids in future dataframe!")
            
            # Check for missing required columns
            required_cols = set(train_data.columns) - {'y'}
            future_cols = set(future_df.columns)
            missing_cols = required_cols - future_cols
            if missing_cols:
                raise ValueError(f"Missing required columns in future dataframe: {missing_cols}")
            
            print(f"ğŸ”® Generating predictions for trial {trial.number}...")
            forecast_df = forecaster.predict(futr_df=future_df)
            
            # Calculate RMSE
            actual = val_data['y'].values
            predicted = forecast_df['NBEATSx'].values[:len(actual)]
            rmse = np.sqrt(mean_squared_error(actual, predicted))
            
            # Store best model
            if rmse < best_rmse:
                best_rmse = rmse
                best_model = forecaster
                print(f"âœ… New best RMSE: {rmse:.6f} (Trial {trial.number})")
            
            trial_time = time.time() - trial_start
            print(f"â±ï¸ Trial {trial.number}/{n_trials} completed in {trial_time:.1f}s - RMSE: {rmse:.6f}")
            
            return rmse
            
        except Exception as e:
            trial_time = time.time() - trial_start
            print(f"âŒ Trial {trial.number} failed after {trial_time:.1f}s: {str(e)}")
            
            # Add more detailed error information
            if "missing combinations" in str(e).lower():
                print("ğŸ” Debugging future dataframe issue:")
                try:
                    print(f"   Train data unique_ids: {train_data['unique_id'].unique()}")
                    print(f"   Train data date range: {train_data['ds'].min()} to {train_data['ds'].max()}")
                    print(f"   Val data unique_ids: {val_data['unique_id'].unique()}")
                    print(f"   Val data date range: {val_data['ds'].min()} to {val_data['ds'].max()}")
                except:
                    pass
            
            return float('inf')
            
        except KeyboardInterrupt:
            print("â¹ï¸ Trial interrupted by user")
            raise
    
    # Run optimization
    print(f"\nğŸš€ Starting optimization with {n_trials} trials...")
    start_time = time.time()
    
    try:
        study.optimize(
            objective,
            n_trials=n_trials,
            timeout=3600,  # 1 hour timeout
            show_progress_bar=True
        )
    except KeyboardInterrupt:
        print("â¹ï¸ Optimization interrupted by user")
    except Exception as e:
        print(f"âš ï¸ Optimization error: {e}")
    
    optimization_time = time.time() - start_time
    
    # Get best parameters
    if len(study.trials) > 0:
        best_params = study.best_params.copy()
        n_blocks = [best_params[f'n_blocks_{i}'] for i in range(3)]
        best_params['n_blocks'] = n_blocks
        
        # Remove individual n_blocks parameters
        for i in range(3):
            best_params.pop(f'n_blocks_{i}', None)
        
        # Results summary
        results = {
            'best_rmse': study.best_value,
            'best_trial_number': study.best_trial.number,
            'total_trials': len(study.trials),
            'optimization_time': optimization_time,
            'study_name': study_name,
            'storage_path': storage_path
        }
        
        print("\n" + "="*70)
        print("ğŸ‰ HYPERPARAMETER OPTIMIZATION COMPLETE")
        print("="*70)
        print(f"ğŸ“Š Total trials: {len(study.trials)}")
        print(f"ğŸ† Best RMSE: {study.best_value:.6f}")
        print(f"âš¡ Best trial: #{study.best_trial.number}")
        print(f"â±ï¸ Total time: {optimization_time:.1f}s")
        print(f"ğŸ’¾ Results saved to: {storage_path}")
        print("\nğŸ“‹ Best hyperparameters:")
        for param, value in best_params.items():
            print(f"   {param}: {value}")
        print("="*70)
        
        return best_params, best_model, results
    
    else:
        print("âŒ No successful trials completed")
        return None, None, None

# =============================================================================
# BLOCK 6: MODEL TRAINING AND EVALUATION
# =============================================================================

def train_final_model(train_data, best_params, feature_lists, horizon=30):
    """
    Train the final model with best hyperparameters.
    
    Args:
        train_data: Training dataset
        best_params (dict): Best hyperparameters from optimization
        feature_lists (dict): Feature specifications
        horizon (int): Forecast horizon
        
    Returns:
        NeuralForecast: Trained forecaster
    """
    
    print("\nğŸ‹ï¸ TRAINING FINAL MODEL WITH BEST PARAMETERS")
    print("="*60)
    
    # Extract features
    future_features = feature_lists['future_features']
    historical_features = feature_lists['historical_features']
    
    # Create model with best parameters
    model = NBEATSx(
        h=horizon,
        input_size=best_params['input_size'],
        futr_exog_list=future_features,
        hist_exog_list=historical_features,
        random_seed=42,
        scaler_type='standard',
        learning_rate=best_params['learning_rate'],
        max_steps=best_params['max_steps'],
        batch_size=best_params['batch_size'],
        stack_types=['identity', 'trend', 'seasonality'],
        n_blocks=best_params['n_blocks'],
        n_harmonics=best_params['n_harmonics'],
        n_polynomials=best_params['n_polynomials'],
        loss=DistributionLoss(distribution='Normal', level=[80, 95])
    )
    
    # Create and train forecaster
    forecaster = NeuralForecast(models=[model], freq='D')
    
    print("ğŸ”„ Training model...")
    start_time = time.time()
    forecaster.fit(df=train_data)
    training_time = time.time() - start_time
    
    print(f"âœ… Model training complete in {training_time:.1f}s")
    print(f"ğŸ“Š Training data: {len(train_data)} samples")
    print(f"ğŸ¯ Model ready for forecasting")
    
    return forecaster

def evaluate_model_performance(forecaster, train_data, test_data, feature_lists, horizon=30):
    """
    Evaluate model performance on test set.
    
    Args:
        forecaster: Trained forecaster
        train_data, test_data: Training and test datasets
        feature_lists (dict): Feature specifications
        horizon (int): Forecast horizon
        
    Returns:
        dict: Evaluation results
    """
    
    print("\nğŸ“ˆ EVALUATING MODEL PERFORMANCE")
    print("="*60)
    
    def create_test_features(train_data, test_data):
        """Create features for test period using robust manual method"""
        print(f"ğŸ”§ Creating test features...")
        print(f"ğŸ”§ Train data shape: {train_data.shape}")
        print(f"ğŸ”§ Test data shape: {test_data.shape}")
        
        # Start with the basic structure from test_data
        future_df = test_data[['ds', 'unique_id']].copy()
        
        # Add time-based features to match future_features
        future_df['dayofweek_sin'] = np.sin(2 * np.pi * future_df['ds'].dt.dayofweek / 7)
        future_df['dayofweek_cos'] = np.cos(2 * np.pi * future_df['ds'].dt.dayofweek / 7)
        print(f"ğŸ”§ Added time-based features")
        
        # Add ALL other columns from training data (except 'y')
        train_columns = set(train_data.columns) - {'y'}
        future_columns = set(future_df.columns)
        missing_columns = train_columns - future_columns
        
        print(f"ğŸ”§ Training columns: {len(train_columns)} - {sorted(train_columns)}")
        print(f"ğŸ”§ Test future columns: {len(future_columns)} - {sorted(future_columns)}")
        print(f"ğŸ”§ Missing columns: {len(missing_columns)} - {sorted(missing_columns)}")
        
        # Add missing columns with values from test_data if available, otherwise use train_data last values
        for col in missing_columns:
            print(f"ğŸ”§ Processing missing column: {col}")
            
            if col in test_data.columns:
                # Use actual values from test_data
                future_df[col] = test_data[col].values
                print(f"ğŸ”§ Added {col} from test_data")
            else:
                # If not available in test_data, use last known values from train_data
                if col in train_data.columns:
                    last_values = train_data.groupby('unique_id')[col].last()
                    future_df[col] = future_df['unique_id'].map(last_values)
                    
                    # Fill any remaining NaNs
                    if future_df[col].isnull().any():
                        if train_data[col].dtype in ['float64', 'int64']:
                            default_val = train_data[col].median() if not train_data[col].isnull().all() else 0
                            future_df[col] = future_df[col].fillna(default_val)
                            print(f"ğŸ”§ Filled NaNs in {col} with median: {default_val}")
                        else:
                            mode_val = train_data[col].mode()
                            default_val = mode_val[0] if len(mode_val) > 0 else 'unknown'
                            future_df[col] = future_df[col].fillna(default_val)
                            print(f"ğŸ”§ Filled NaNs in {col} with mode: {default_val}")
                else:
                    # Column doesn't exist anywhere, use default
                    future_df[col] = 0
                    print(f"ğŸ”§ Added new column {col} with default: 0")
        
        # Ensure proper column order matching training data
        train_col_order = [col for col in train_data.columns if col != 'y']
        
        # Verify all required columns exist
        missing_cols = set(train_col_order) - set(future_df.columns)
        if missing_cols:
            print(f"âš ï¸ Still missing columns: {missing_cols}")
            for col in missing_cols:
                future_df[col] = 0
                print(f"ğŸ”§ Added missing column {col} with value 0")
        
        # Reorder columns to match training data exactly
        try:
            future_df = future_df[train_col_order]
            print(f"ğŸ”§ Column reordering successful")
        except KeyError as e:
            print(f"âš ï¸ Column reordering failed: {e}")
            # Use intersection of available columns
            available_cols = [col for col in train_col_order if col in future_df.columns]
            future_df = future_df[available_cols]
            print(f"ğŸ”§ Using available columns: {len(available_cols)}")
        
        # Final data type consistency check
        for col in future_df.columns:
            if col in train_data.columns:
                if train_data[col].dtype != future_df[col].dtype:
                    try:
                        future_df[col] = future_df[col].astype(train_data[col].dtype)
                        print(f"ğŸ”§ Converted {col} to {train_data[col].dtype}")
                    except:
                        print(f"âš ï¸ Could not convert {col} to {train_data[col].dtype}")
        
        # Handle nulls
        null_cols = future_df.columns[future_df.isnull().any()].tolist()
        if null_cols:
            print(f"ğŸ”§ Handling nulls in columns: {null_cols}")
            for col in null_cols:
                null_count = future_df[col].isnull().sum()
                if future_df[col].dtype in ['float64', 'int64']:
                    future_df[col] = future_df[col].fillna(0)
                else:
                    future_df[col] = future_df[col].fillna('unknown')
                print(f"ğŸ”§ Filled {null_count} nulls in {col}")
        
        # Final validation
        print(f"ğŸ”§ Test future dataframe: {future_df.shape} with columns: {list(future_df.columns)}")
        print(f"ğŸ”§ Training data columns: {list(train_data.columns)}")
        
        # Check feature alignment
        train_features = set(train_data.columns) - {'y'}
        future_features = set(future_df.columns)
        features_match = train_features == future_features
        print(f"ğŸ”§ Test features match perfectly: {features_match}")
        
        if not features_match:
            print(f"âš ï¸ Test feature mismatch!")
            print(f"   Missing: {train_features - future_features}")
            print(f"   Extra: {future_features - train_features}")
        
        return future_df
    
    # Generate predictions
    print("ğŸ”® Generating predictions...")
    future_df = create_test_features(train_data, test_data)
    forecast_df = forecaster.predict(futr_df=future_df)
    
    # Calculate metrics
    actual_values = test_data['y'].values
    predicted_values = forecast_df['NBEATSx'].values
    
    # Ensure same length
    min_len = min(len(actual_values), len(predicted_values))
    actual_values = actual_values[:min_len]
    predicted_values = predicted_values[:min_len]
    
    # Calculate metrics
    mae = mean_absolute_error(actual_values, predicted_values)
    mse = mean_squared_error(actual_values, predicted_values)
    rmse = np.sqrt(mse)
    
    # Additional metrics
    mape = np.mean(np.abs((actual_values - predicted_values) / np.maximum(np.abs(actual_values), 1e-8))) * 100
    correlation = np.corrcoef(actual_values, predicted_values)[0, 1] if len(actual_values) > 1 else 0
    r2 = r2_score(actual_values, predicted_values)
    
    # Residual analysis
    residuals = actual_values - predicted_values
    residual_mean = np.mean(residuals)
    residual_std = np.std(residuals)
    
    results = {
        'metrics': {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape,
            'correlation': correlation,
            'r2_score': r2
        },
        'residuals': {
            'mean': residual_mean,
            'std': residual_std,
            'values': residuals
        },
        'predictions': {
            'actual': actual_values,
            'predicted': predicted_values,
            'dates': test_data['ds'].values[:min_len]
        },
        'forecast_df': forecast_df
    }
    
    # Print results
    print("ğŸ“Š PERFORMANCE METRICS:")
    print(f"   MAE:         {mae:.6f}")
    print(f"   RMSE:        {rmse:.6f}")
    print(f"   MAPE:        {mape:.2f}%")
    print(f"   Correlation: {correlation:.6f}")
    print(f"   RÂ² Score:    {r2:.6f}")
    
    print(f"\nğŸ“Š RESIDUAL ANALYSIS:")
    print(f"   Mean:        {residual_mean:.6f}")
    print(f"   Std Dev:     {residual_std:.6f}")
    
    # Quality assessment
    if rmse < 0.1:
        quality = "Excellent"
    elif rmse < 0.2:
        quality = "Good"
    elif rmse < 0.5:
        quality = "Fair"
    else:
        quality = "Needs Improvement"
    
    print(f"\nğŸ¯ MODEL QUALITY: {quality}")
    print("="*60)
    
    return results

# =============================================================================
# BLOCK 7: COMPREHENSIVE VISUALIZATION
# =============================================================================

def create_comprehensive_visualizations(evaluation_results, optimization_results=None, 
                                       save_dir="./visualizations"):
    """
    Create comprehensive visualizations for the balance prediction results.
    
    Args:
        evaluation_results (dict): Model evaluation results
        optimization_results (dict): Hyperparameter optimization results
        save_dir (str): Directory to save plots
    """
    
    print("\nğŸ“Š CREATING COMPREHENSIVE VISUALIZATIONS")
    print("="*60)
    
    # Create save directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Set style
    plt.style.use('default')
    sns.set_palette("husl")
    
    # 1. Actual vs Predicted Time Series Plot
    print("ğŸ“ˆ Creating actual vs predicted plot...")
    
    fig, ax = plt.subplots(figsize=(15, 8))
    
    dates = evaluation_results['predictions']['dates']
    actual = evaluation_results['predictions']['actual']
    predicted = evaluation_results['predictions']['predicted']
    
    # Plot time series
    ax.plot(dates, actual, label='Actual Balance', linewidth=2, color='#2E86AB', alpha=0.8)
    ax.plot(dates, predicted, label='Predicted Balance', linewidth=2, color='#A23B72', alpha=0.8)
    
    # Add confidence intervals if available
    if 'NBEATSx-lo-95' in evaluation_results['forecast_df'].columns:
        lo_95 = evaluation_results['forecast_df']['NBEATSx-lo-95'].values[:len(dates)]
        hi_95 = evaluation_results['forecast_df']['NBEATSx-hi-95'].values[:len(dates)]
        ax.fill_between(dates, lo_95, hi_95, alpha=0.2, color='#A23B72', label='95% Confidence Interval')
    
    # Formatting
    ax.set_title('Balance Prediction: Actual vs Predicted', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('Date', fontsize=12)
    ax.set_ylabel('Normalized Balance', fontsize=12)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # Add metrics text box
    metrics = evaluation_results['metrics']
    textstr = f"""Performance Metrics:
RMSE: {metrics['rmse']:.6f}
MAE: {metrics['mae']:.6f}
MAPE: {metrics['mape']:.2f}%
Correlation: {metrics['correlation']:.6f}
RÂ²: {metrics['r2_score']:.6f}"""
    
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{save_dir}/actual_vs_predicted.png", dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_dir}/actual_vs_predicted.png")
    
    # 2. Residual Analysis Plot
    print("ğŸ“Š Creating residual analysis plot...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    residuals = evaluation_results['residuals']['values']
    
    # Residuals over time
    ax1.plot(dates, residuals, color='#F18F01', alpha=0.7)
    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax1.set_title('Residuals Over Time')
    ax1.set_xlabel('Date')
    ax1.set_ylabel('Residuals')
    ax1.grid(True, alpha=0.3)
    
    # Residual histogram
    ax2.hist(residuals, bins=30, color='#C73E1D', alpha=0.7, edgecolor='black')
    ax2.set_title('Residual Distribution')
    ax2.set_xlabel('Residuals')
    ax2.set_ylabel('Frequency')
    ax2.grid(True, alpha=0.3)
    
    # Q-Q plot
    stats.probplot(residuals, dist="norm", plot=ax3)
    ax3.set_title('Q-Q Plot (Normal Distribution)')
    ax3.grid(True, alpha=0.3)
    
    # Predicted vs Residuals
    ax4.scatter(predicted, residuals, alpha=0.6, color='#2E86AB')
    ax4.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax4.set_title('Predicted vs Residuals')
    ax4.set_xlabel('Predicted Values')
    ax4.set_ylabel('Residuals')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/residual_analysis.png", dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_dir}/residual_analysis.png")
    
    # 3. Scatter Plot: Actual vs Predicted
    print("ğŸ¯ Creating scatter plot...")
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create scatter plot
    ax.scatter(actual, predicted, alpha=0.6, s=50, color='#2E86AB')
    
    # Perfect prediction line
    min_val = min(min(actual), min(predicted))
    max_val = max(max(actual), max(predicted))
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
    
    # Calculate and display RÂ²
    r2 = metrics['r2_score']
    ax.text(0.05, 0.95, f'RÂ² = {r2:.6f}', transform=ax.transAxes, fontsize=14,
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    ax.set_xlabel('Actual Balance', fontsize=12)
    ax.set_ylabel('Predicted Balance', fontsize=12)
    ax.set_title('Actual vs Predicted Balance (Scatter Plot)', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/scatter_actual_vs_predicted.png", dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_dir}/scatter_actual_vs_predicted.png")
    
    # 4. Model Performance Summary
    print("ğŸ“‹ Creating performance summary...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.axis('off')
    
    # Create summary text
    summary_text = f"""
    NBEATS BALANCE PREDICTION - MODEL PERFORMANCE SUMMARY
    =====================================================
    
    ğŸ“Š DATASET INFORMATION:
    â€¢ Test Period: {len(actual)} days
    â€¢ Date Range: {dates[0].strftime('%Y-%m-%d')} to {dates[-1].strftime('%Y-%m-%d')}
    
    ğŸ“ˆ PERFORMANCE METRICS:
    â€¢ Root Mean Square Error (RMSE): {metrics['rmse']:.6f}
    â€¢ Mean Absolute Error (MAE): {metrics['mae']:.6f}
    â€¢ Mean Absolute Percentage Error (MAPE): {metrics['mape']:.2f}%
    â€¢ Correlation Coefficient: {metrics['correlation']:.6f}
    â€¢ RÂ² Score: {metrics['r2_score']:.6f}
    
    ğŸ“Š RESIDUAL STATISTICS:
    â€¢ Residual Mean: {evaluation_results['residuals']['mean']:.6f}
    â€¢ Residual Std Dev: {evaluation_results['residuals']['std']:.6f}
    
    ğŸ¯ MODEL QUALITY ASSESSMENT:
    """
    
    # Add quality assessment
    if metrics['rmse'] < 0.1:
        summary_text += "â€¢ Overall Quality: EXCELLENT âœ…\n"
        summary_text += "â€¢ Model shows high accuracy and reliability\n"
    elif metrics['rmse'] < 0.2:
        summary_text += "â€¢ Overall Quality: GOOD âœ…\n"
        summary_text += "â€¢ Model performs well for most predictions\n"
    elif metrics['rmse'] < 0.5:
        summary_text += "â€¢ Overall Quality: FAIR âš ï¸\n"
        summary_text += "â€¢ Model shows moderate performance\n"
    else:
        summary_text += "â€¢ Overall Quality: NEEDS IMPROVEMENT âŒ\n"
        summary_text += "â€¢ Consider parameter tuning or feature engineering\n"
    
    if optimization_results:
        summary_text += f"""
    âš™ï¸ OPTIMIZATION DETAILS:
    â€¢ Total Trials: {optimization_results['total_trials']}
    â€¢ Best Trial: #{optimization_results['best_trial_number']}
    â€¢ Optimization Time: {optimization_results['optimization_time']:.1f} seconds
    â€¢ Storage: {optimization_results['storage_path']}
    """
    
    summary_text += f"""
    
    ğŸ“… Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    """
    
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round,pad=1', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/performance_summary.png", dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_dir}/performance_summary.png")
    
    # Close all plots to free memory
    plt.close('all')
    
    print(f"\nğŸ‰ All visualizations saved to: {save_dir}")
    print("ğŸ“Š Generated plots:")
    print("   â€¢ actual_vs_predicted.png - Time series comparison")
    print("   â€¢ residual_analysis.png - Comprehensive residual analysis")
    print("   â€¢ scatter_actual_vs_predicted.png - Scatter plot with RÂ²")
    print("   â€¢ performance_summary.png - Complete model summary")
    print("="*60)

# =============================================================================
# BLOCK 8: MAIN EXECUTION PIPELINE
# =============================================================================

def main_pipeline(data_file="/kaggle/input/shiharadataset/featured_shihara.xlsx", 
                 n_trials=20, 
                 test_days=30, 
                 horizon=30):
    """
    Execute the complete NBEATS balance prediction pipeline.
    
    Args:
        data_file (str): Path to input data file
        n_trials (int): Number of hyperparameter optimization trials
        test_days (int): Number of days for test set
        horizon (int): Forecast horizon
    """
    
    print("ğŸš€ STARTING COMPLETE NBEATS BALANCE PREDICTION PIPELINE")
    print("="*80)
    print(f"ğŸ“‚ Data file: {data_file}")
    print(f"ğŸ¯ Optimization trials: {n_trials}")
    print(f"ğŸ§ª Test days: {test_days}")
    print(f"ğŸ”® Forecast horizon: {horizon}")
    print("="*80)
    
    pipeline_start = time.time()
    
    try:
        # BLOCK 1: Load and inspect data
        df = load_and_inspect_data(data_file)
        if df is None:
            return None
        
        # BLOCK 2: Handle null values
        null_analysis = detect_null_patterns(df)
        df_clean = handle_null_values(df, method='auto', verbose=True)
        
        # BLOCK 3: Split data
        train_df, val_df, test_df, split_info = prepare_train_test_split(
            df_clean, test_days=test_days, validation_days=15, verbose=True
        )
        
        # BLOCK 4: Prepare for NeuralForecast
        train_data, val_data, test_data, feature_lists = prepare_neuralforecast_data(
            train_df, val_df, test_df, verbose=True
        )
        
        # BLOCK 5: Hyperparameter optimization
        best_params, best_model, optimization_results = hyperparameter_optimization(
            train_data, val_data, feature_lists,
            horizon=horizon, n_trials=n_trials
        )
        
        if best_params is None:
            print("âŒ Hyperparameter optimization failed")
            return None
        
        # BLOCK 6: Evaluate model (use pre-trained model from optimization)
        evaluation_results = evaluate_model_performance(
            best_model, train_data, test_data, feature_lists, horizon=horizon
        )
        
        # BLOCK 7: Create visualizations
        create_comprehensive_visualizations(
            evaluation_results, optimization_results, save_dir="./visualizations"
        )
        
        # Pipeline summary
        pipeline_time = time.time() - pipeline_start
        
        print("\n" + "="*80)
        print("ğŸ‰ PIPELINE EXECUTION COMPLETE")
        print("="*80)
        print(f"â±ï¸ Total execution time: {pipeline_time:.1f} seconds")
        print(f"ğŸ“Š Final RMSE: {evaluation_results['metrics']['rmse']:.6f}")
        print(f"ğŸ“Š Final RÂ²: {evaluation_results['metrics']['r2_score']:.6f}")
        print(f"ğŸ’¾ Results saved to: ./visualizations/")
        print(f"ğŸ’¾ Optimization data: ./optuna_balance_study.db")
        
        # Return comprehensive results
        pipeline_results = {
            'data_info': {
                'original_shape': df.shape,
                'clean_shape': df_clean.shape,
                'split_info': split_info
            },
            'null_analysis': null_analysis,
            'best_parameters': best_params,
            'optimization_results': optimization_results,
            'evaluation_results': evaluation_results,
            'execution_time': pipeline_time
        }
        
        print("âœ… All results compiled and ready")
        print("="*80)
        
        return pipeline_results
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        print(f"âŒ Pipeline failed: {str(e)}")
        return None

# =============================================================================
# BLOCK 9: SCRIPT EXECUTION
# =============================================================================

if __name__ == "__main__":
    """
    Execute the complete pipeline when script is run directly.
    """
    
    print("ğŸ¬ EXECUTING NBEATS BALANCE PREDICTION SYSTEM")
    print("="*80)
    
    # Configuration
    CONFIG = {
        'data_file': "/kaggle/input/shiharadataset/featured_shihara.xlsx",
        'n_trials': 15,  # Adjust based on time constraints
        'test_days': 30,
        'horizon': 30
    }
    
    print("ğŸ“‹ CONFIGURATION:")
    for key, value in CONFIG.items():
        print(f"   {key}: {value}")
    print("="*80)
    
    # Execute pipeline
    results = main_pipeline(**CONFIG)
    
    if results:
        print("\nğŸŠ SUCCESS! Balance prediction pipeline completed successfully!")
        print("ğŸ“ Check the following outputs:")
        print("   â€¢ ./visualizations/ - All generated plots")
        print("   â€¢ ./optuna_balance_study.db - Hyperparameter optimization results")
        print("   â€¢ Console output - Detailed performance metrics")
    else:
        print("\nâŒ Pipeline execution failed. Check error messages above.")
    
    print("\n" + "="*80)
    print("ğŸ SCRIPT EXECUTION COMPLETE")
    print("="*80)

# =============================================================================
# END OF SCRIPT
# =============================================================================
