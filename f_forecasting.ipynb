{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d34bd7e",
   "metadata": {},
   "source": [
    "# NBEATSx Account Balance Forecasting - Next 30 Days\n",
    "\n",
    "This notebook implements a complete forecasting system to predict account balance for the next 30 days using NBEATSx neural network model with optimized hyperparameters.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Model**: NBEATSx (Neural Basis Expansion Analysis for Time Series Forecasting)\n",
    "- **Objective**: Predict account balance for the next 30 days\n",
    "- **Data**: Preprocessed training dataset with engineered features\n",
    "- **Optimization**: Optuna hyperparameter tuning with 11-hour timeout\n",
    "- **Database**: SQLite for Optuna study storage\n",
    "- **Features**: Time-based features, lags, rolling statistics\n",
    "\n",
    "## Workflow\n",
    "1. **Data Import** - Load preprocessed dataset\n",
    "2. **Data Validation** - Verify data quality and completeness\n",
    "3. **Hyperparameter Tuning** - Optimize model parameters using Optuna\n",
    "4. **Model Training** - Train final model with best parameters\n",
    "5. **Forecasting** - Generate 30-day predictions with uncertainty intervals\n",
    "6. **Evaluation & Analysis** - Analyze results and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6dbc8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import all required libraries for data processing, modeling, visualization, and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series and forecasting\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATSx\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import sqlite3\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ðŸ”§ Optuna version: {optuna.__version__}\")\n",
    "print(\"ðŸš€ Ready for forecasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0eb98",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Validation\n",
    "\n",
    "Load the preprocessed dataset and validate data quality. The dataset should already be preprocessed with features engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e08f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset\n",
    "print(\"ðŸ“‚ Loading preprocessed dataset...\")\n",
    "df = pd.read_excel(\"processed_train_dataset.xlsx\")\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"ðŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"ðŸ“… Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"ðŸ“ˆ Total days: {len(df)}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nðŸ“‹ Dataset Info:\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nâŒ Missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"   {col}: {missing} ({missing/len(df)*100:.2f}%)\")\n",
    "    \n",
    "if missing_values.sum() == 0:\n",
    "    print(\"   âœ… No missing values found!\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Total missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f45828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation and feature identification\n",
    "print(\"ðŸ” Data Validation and Feature Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validate essential columns\n",
    "required_columns = ['Date', 'Normalized_Balance']\n",
    "missing_required = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_required:\n",
    "    print(f\"âŒ Missing required columns: {missing_required}\")\n",
    "    raise ValueError(f\"Dataset must contain columns: {required_columns}\")\n",
    "else:\n",
    "    print(\"âœ… All required columns present\")\n",
    "\n",
    "# Identify feature types\n",
    "feature_categories = {\n",
    "    'date_column': 'Date',\n",
    "    'target_column': 'Normalized_Balance',\n",
    "    'future_features': [],\n",
    "    'historical_features': []\n",
    "}\n",
    "\n",
    "# Categorize features automatically\n",
    "for col in df.columns:\n",
    "    if col in required_columns:\n",
    "        continue\n",
    "    elif any(x in col.lower() for x in ['dayofweek_sin', 'dayofweek_cos', 'sin', 'cos']):\n",
    "        feature_categories['future_features'].append(col)\n",
    "    elif any(x in col.lower() for x in ['ago', 'lag', 'rolling', 'mean', 'std', 'changed']):\n",
    "        feature_categories['historical_features'].append(col)\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Categories:\")\n",
    "print(f\"ðŸŽ¯ Target: {feature_categories['target_column']}\")\n",
    "print(f\"ðŸ”® Future features ({len(feature_categories['future_features'])}): {feature_categories['future_features']}\")\n",
    "print(f\"ðŸ“ˆ Historical features ({len(feature_categories['historical_features'])}): {feature_categories['historical_features']}\")\n",
    "\n",
    "# Validate data quality\n",
    "print(f\"\\nðŸ” Data Quality Checks:\")\n",
    "\n",
    "# Check date continuity\n",
    "df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
    "date_diff = df_sorted['Date'].diff().dt.days\n",
    "missing_dates = (date_diff > 1).sum()\n",
    "print(f\"ðŸ“… Date continuity: {len(df_sorted) - missing_dates - 1}/{len(df_sorted) - 1} consecutive days\")\n",
    "\n",
    "# Check target variable\n",
    "target_stats = df[feature_categories['target_column']].describe()\n",
    "print(f\"ðŸ’° Target variable stats:\")\n",
    "print(f\"   Range: {target_stats['min']:.4f} to {target_stats['max']:.4f}\")\n",
    "print(f\"   Mean: {target_stats['mean']:.4f}, Std: {target_stats['std']:.4f}\")\n",
    "\n",
    "# Check for outliers (values beyond 3 standard deviations)\n",
    "target_col = feature_categories['target_column']\n",
    "mean_val = df[target_col].mean()\n",
    "std_val = df[target_col].std()\n",
    "outliers = ((df[target_col] - mean_val).abs() > 3 * std_val).sum()\n",
    "print(f\"âš ï¸ Potential outliers (>3Ïƒ): {outliers} ({outliers/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Data validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d00512",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning with Optuna\n",
    "\n",
    "Optimize NBEATSx model hyperparameters using Optuna with SQLite storage and 11-hour timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Optuna study with SQLite storage\n",
    "print(\"ðŸ”§ Setting up Optuna hyperparameter optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "HORIZON = 30  # Forecast horizon (days)\n",
    "TIMEOUT_HOURS = 11  # 11 hours timeout\n",
    "TIMEOUT_SECONDS = TIMEOUT_HOURS * 3600\n",
    "STUDY_NAME = \"nbeats_balance_forecasting\"\n",
    "DB_URL = f\"sqlite:///optuna_study_{STUDY_NAME}.db\"\n",
    "\n",
    "print(f\"ðŸŽ¯ Forecast horizon: {HORIZON} days\")\n",
    "print(f\"â° Timeout: {TIMEOUT_HOURS} hours ({TIMEOUT_SECONDS} seconds)\")\n",
    "print(f\"ðŸ’¾ Database: {DB_URL}\")\n",
    "\n",
    "# Create or load existing study\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name=STUDY_NAME,\n",
    "        storage=DB_URL,\n",
    "        direction='minimize',\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    print(f\"âœ… Study loaded/created: {len(study.trials)} existing trials\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Creating new study: {e}\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=STUDY_NAME,\n",
    "        storage=DB_URL,\n",
    "        direction='minimize'\n",
    "    )\n",
    "\n",
    "# Split data for hyperparameter tuning (use last 20% for validation)\n",
    "VALIDATION_SIZE = 0.2\n",
    "split_idx = int(len(df) * (1 - VALIDATION_SIZE))\n",
    "train_data = df.iloc[:split_idx].copy()\n",
    "val_data = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Training data: {len(train_data)} days ({df['Date'].iloc[0]} to {df['Date'].iloc[split_idx-1]})\")\n",
    "print(f\"ðŸ“Š Validation data: {len(val_data)} days ({df['Date'].iloc[split_idx]} to {df['Date'].iloc[-1]})\")\n",
    "\n",
    "# Prepare data for NeuralForecast format\n",
    "def prepare_neural_forecast_data(data, feature_categories):\n",
    "    \"\"\"Convert data to NeuralForecast format\"\"\"\n",
    "    nf_data = data.copy()\n",
    "    nf_data['unique_id'] = 'balance'\n",
    "    nf_data = nf_data.rename(columns={\n",
    "        feature_categories['date_column']: 'ds', \n",
    "        feature_categories['target_column']: 'y'\n",
    "    })\n",
    "    return nf_data\n",
    "\n",
    "train_nf = prepare_neural_forecast_data(train_data, feature_categories)\n",
    "val_nf = prepare_neural_forecast_data(val_data, feature_categories)\n",
    "\n",
    "print(\"âœ… Data prepared for optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future features creation function\n",
    "def create_future_features(last_date, horizon, feature_categories):\n",
    "    \"\"\"\n",
    "    Create future features for forecasting period.\n",
    "    Only creates features that can be known in advance (like time-based features).\n",
    "    \"\"\"\n",
    "    # Generate future dates\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_date + pd.Timedelta(days=1), \n",
    "        periods=horizon, \n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    # Create future dataframe\n",
    "    future_df = pd.DataFrame({\n",
    "        'ds': future_dates,\n",
    "        'unique_id': 'balance'\n",
    "    })\n",
    "    \n",
    "    # Add time-based features (these can be known in advance)\n",
    "    future_df['dayofweek_sin'] = np.sin(2 * np.pi * future_df['ds'].dt.dayofweek / 7)\n",
    "    future_df['dayofweek_cos'] = np.cos(2 * np.pi * future_df['ds'].dt.dayofweek / 7)\n",
    "    \n",
    "    # Add any other future features that exist in the dataset\n",
    "    for feature in feature_categories['future_features']:\n",
    "        if feature not in future_df.columns:\n",
    "            if 'sin' in feature.lower():\n",
    "                # Handle additional sine features\n",
    "                if 'month' in feature.lower():\n",
    "                    future_df[feature] = np.sin(2 * np.pi * future_df['ds'].dt.month / 12)\n",
    "                elif 'dayofyear' in feature.lower():\n",
    "                    future_df[feature] = np.sin(2 * np.pi * future_df['ds'].dt.dayofyear / 365.25)\n",
    "                else:\n",
    "                    future_df[feature] = 0  # Default value\n",
    "            elif 'cos' in feature.lower():\n",
    "                # Handle additional cosine features\n",
    "                if 'month' in feature.lower():\n",
    "                    future_df[feature] = np.cos(2 * np.pi * future_df['ds'].dt.month / 12)\n",
    "                elif 'dayofyear' in feature.lower():\n",
    "                    future_df[feature] = np.cos(2 * np.pi * future_df['ds'].dt.dayofyear / 365.25)\n",
    "                else:\n",
    "                    future_df[feature] = 0  # Default value\n",
    "            else:\n",
    "                future_df[feature] = 0  # Default for unknown future features\n",
    "    \n",
    "    print(f\"ðŸ”® Created future features for {horizon} days\")\n",
    "    print(f\"ðŸ“… Future period: {future_dates[0]} to {future_dates[-1]}\")\n",
    "    \n",
    "    return future_df\n",
    "\n",
    "# Test future features creation\n",
    "last_training_date = train_nf['ds'].max()\n",
    "test_future_df = create_future_features(last_training_date, 5, feature_categories)\n",
    "print(\"\\nðŸ“‹ Sample future features:\")\n",
    "print(test_future_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for NBEATSx hyperparameter optimization.\n",
    "    Returns RMSE score to minimize.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Hyperparameter search space\n",
    "        params = {\n",
    "            'input_size': trial.suggest_int('input_size', 60, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'max_steps': trial.suggest_int('max_steps', 500, 2000),\n",
    "            'batch_size': trial.suggest_int('batch_size', 16, 64),\n",
    "            'n_harmonics': trial.suggest_int('n_harmonics', 1, 5),\n",
    "            'n_polynomials': trial.suggest_int('n_polynomials', 1, 5),\n",
    "            'dropout_prob_theta': trial.suggest_float('dropout_prob_theta', 0.0, 0.3),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "            'early_stop_patience_steps': trial.suggest_int('early_stop_patience_steps', 20, 100)\n",
    "        }\n",
    "        \n",
    "        # N-blocks for each stack (identity, trend, seasonality)\n",
    "        n_blocks = [\n",
    "            trial.suggest_int('n_blocks_identity', 2, 6),\n",
    "            trial.suggest_int('n_blocks_trend', 2, 6), \n",
    "            trial.suggest_int('n_blocks_seasonality', 2, 6)\n",
    "        ]\n",
    "        \n",
    "        # Create NBEATSx model\n",
    "        model = NBEATSx(\n",
    "            h=HORIZON,\n",
    "            input_size=params['input_size'],\n",
    "            futr_exog_list=feature_categories['future_features'],\n",
    "            hist_exog_list=feature_categories['historical_features'],\n",
    "            \n",
    "            # Architecture parameters\n",
    "            stack_types=['identity', 'trend', 'seasonality'],\n",
    "            n_blocks=n_blocks,\n",
    "            n_harmonics=params['n_harmonics'],\n",
    "            n_polynomials=params['n_polynomials'],\n",
    "            \n",
    "            # Training parameters\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_steps=params['max_steps'],\n",
    "            batch_size=params['batch_size'],\n",
    "            dropout_prob_theta=params['dropout_prob_theta'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            early_stop_patience_steps=params['early_stop_patience_steps'],\n",
    "            \n",
    "            # Other settings\n",
    "            random_seed=42,\n",
    "            scaler_type='standard',\n",
    "            loss=DistributionLoss(distribution='Normal', level=[80, 90, 95]),\n",
    "            \n",
    "            # Reduce verbosity for optimization\n",
    "            trainer_kwargs={\n",
    "                'enable_progress_bar': False,\n",
    "                'enable_model_summary': False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create forecaster\n",
    "        forecaster = NeuralForecast(models=[model], freq='D')\n",
    "        \n",
    "        # Fit model on training data\n",
    "        forecaster.fit(df=train_nf)\n",
    "        \n",
    "        # Create future features for validation period\n",
    "        future_df = create_future_features(\n",
    "            last_date=train_nf['ds'].max(),\n",
    "            horizon=len(val_nf),\n",
    "            feature_categories=feature_categories\n",
    "        )\n",
    "        \n",
    "        # Generate predictions\n",
    "        forecast_df = forecaster.predict(futr_df=future_df)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        actual = val_nf['y'].values\n",
    "        predicted = forecast_df['NBEATSx'].values\n",
    "        \n",
    "        # Handle any potential NaN values\n",
    "        mask = ~(np.isnan(actual) | np.isnan(predicted))\n",
    "        if mask.sum() == 0:\n",
    "            return float('inf')\n",
    "            \n",
    "        rmse = np.sqrt(mean_squared_error(actual[mask], predicted[mask]))\n",
    "        \n",
    "        # Log trial info\n",
    "        trial_info = {\n",
    "            'trial_number': trial.number,\n",
    "            'rmse': rmse,\n",
    "            'params': params,\n",
    "            'n_blocks': n_blocks\n",
    "        }\n",
    "        \n",
    "        print(f\"Trial {trial.number}: RMSE = {rmse:.6f}\")\n",
    "        \n",
    "        return rmse\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "        return float('inf')\n",
    "\n",
    "print(\"ðŸŽ¯ Objective function defined!\")\n",
    "print(\"âš™ï¸ Hyperparameter search space:\")\n",
    "print(\"   - input_size: 60-200\")\n",
    "print(\"   - learning_rate: 1e-4 to 1e-2 (log scale)\")\n",
    "print(\"   - max_steps: 500-2000\")\n",
    "print(\"   - batch_size: 16-64\")\n",
    "print(\"   - n_blocks per stack: 2-6 each\")\n",
    "print(\"   - n_harmonics: 1-5\")\n",
    "print(\"   - n_polynomials: 1-5\")\n",
    "print(\"   - dropout_prob_theta: 0.0-0.3\")\n",
    "print(\"   - weight_decay: 1e-6 to 1e-3 (log scale)\")\n",
    "print(\"   - early_stop_patience_steps: 20-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad03445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimization\n",
    "print(\"ðŸš€ Starting hyperparameter optimization...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"â° Timeout: {TIMEOUT_HOURS} hours\")\n",
    "print(f\"ðŸ’¾ Results will be saved to: {DB_URL}\")\n",
    "print(f\"ðŸ”„ Existing trials: {len(study.trials)}\")\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "print(f\"ðŸ• Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run optimization with timeout\n",
    "try:\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        timeout=TIMEOUT_SECONDS,\n",
    "        n_jobs=1,  # Single job to avoid conflicts\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    optimization_completed = True\n",
    "    print(\"\\nâœ… Optimization completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸ Optimization interrupted by user\")\n",
    "    optimization_completed = False\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Optimization failed: {e}\")\n",
    "    optimization_completed = False\n",
    "\n",
    "# Record end time and duration\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(f\"ðŸ• End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"â±ï¸ Duration: {duration}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(study.trials) > 0:\n",
    "    print(f\"ðŸ”„ Total trials completed: {len(study.trials)}\")\n",
    "    print(f\"ðŸ† Best RMSE: {study.best_value:.6f}\")\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params.copy()\n",
    "    \n",
    "    # Reconstruct n_blocks array\n",
    "    n_blocks_best = [\n",
    "        best_params.pop('n_blocks_identity'),\n",
    "        best_params.pop('n_blocks_trend'),\n",
    "        best_params.pop('n_blocks_seasonality')\n",
    "    ]\n",
    "    best_params['n_blocks'] = n_blocks_best\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Best hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    with open('best_hyperparameters.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=2, default=str)\n",
    "    print(f\"\\nðŸ’¾ Best parameters saved to: best_hyperparameters.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No trials completed\")\n",
    "    best_params = None\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimization results\n",
    "if len(study.trials) > 0:\n",
    "    print(\"ðŸ“ˆ OPTIMIZATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create trials dataframe for analysis\n",
    "    trials_df = study.trials_dataframe()\n",
    "    completed_trials = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "    \n",
    "    if len(completed_trials) > 0:\n",
    "        print(f\"âœ… Completed trials: {len(completed_trials)}\")\n",
    "        print(f\"âŒ Failed trials: {len(trials_df) - len(completed_trials)}\")\n",
    "        print(f\"ðŸ“Š Success rate: {len(completed_trials)/len(trials_df)*100:.1f}%\")\n",
    "        \n",
    "        # Statistics\n",
    "        rmse_stats = completed_trials['value'].describe()\n",
    "        print(f\"\\nðŸ“Š RMSE Statistics:\")\n",
    "        print(f\"   Best (min): {rmse_stats['min']:.6f}\")\n",
    "        print(f\"   Worst (max): {rmse_stats['max']:.6f}\")\n",
    "        print(f\"   Mean: {rmse_stats['mean']:.6f}\")\n",
    "        print(f\"   Std: {rmse_stats['std']:.6f}\")\n",
    "        print(f\"   Median: {rmse_stats['50%']:.6f}\")\n",
    "        \n",
    "        # Plot optimization history\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Optimization history\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(completed_trials['number'], completed_trials['value'], 'b-', alpha=0.7)\n",
    "        plt.axhline(y=study.best_value, color='r', linestyle='--', label=f'Best RMSE: {study.best_value:.6f}')\n",
    "        plt.xlabel('Trial Number')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('Optimization History')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: RMSE distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist(completed_trials['value'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.6f}')\n",
    "        plt.xlabel('RMSE')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('RMSE Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Parameter importance (if enough trials)\n",
    "        if len(completed_trials) >= 10:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            try:\n",
    "                importance = optuna.importance.get_param_importances(study)\n",
    "                params = list(importance.keys())[:8]  # Top 8 parameters\n",
    "                values = [importance[p] for p in params]\n",
    "                \n",
    "                plt.barh(params, values)\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title('Parameter Importance')\n",
    "                plt.gca().invert_yaxis()\n",
    "            except:\n",
    "                plt.text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Parameter Importance')\n",
    "        \n",
    "        # Plot 4: Learning curve of best trial\n",
    "        plt.subplot(2, 2, 4)\n",
    "        # This would show learning curve if we had access to training history\n",
    "        plt.text(0.5, 0.5, f'Best Trial: #{study.best_trial.number}\\nRMSE: {study.best_value:.6f}', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "        plt.title('Best Trial Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save optimization report\n",
    "        optimization_report = {\n",
    "            'study_name': STUDY_NAME,\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len(completed_trials),\n",
    "            'best_rmse': study.best_value,\n",
    "            'best_params': best_params,\n",
    "            'optimization_duration': str(duration),\n",
    "            'rmse_statistics': rmse_stats.to_dict()\n",
    "        }\n",
    "        \n",
    "        with open('optimization_report.json', 'w') as f:\n",
    "            json.dump(optimization_report, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Optimization report saved to: optimization_report.json\")\n",
    "    else:\n",
    "        print(\"âŒ No completed trials to analyze\")\n",
    "else:\n",
    "    print(\"âŒ No trials to analyze\")\n",
    "    best_params = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58005d",
   "metadata": {},
   "source": [
    "## 4. Final Model Training\n",
    "\n",
    "Train the final NBEATSx model using the best hyperparameters found during optimization, using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "print(\"ðŸš€ Training Final Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure we have best parameters\n",
    "if best_params is None:\n",
    "    print(\"âš ï¸ No optimized parameters found, using default parameters\")\n",
    "    best_params = {\n",
    "        'input_size': 120,\n",
    "        'learning_rate': 0.001,\n",
    "        'max_steps': 1000,\n",
    "        'batch_size': 32,\n",
    "        'n_blocks': [3, 3, 3],\n",
    "        'n_harmonics': 2,\n",
    "        'n_polynomials': 2,\n",
    "        'dropout_prob_theta': 0.1,\n",
    "        'weight_decay': 1e-4,\n",
    "        'early_stop_patience_steps': 50\n",
    "    }\n",
    "\n",
    "print(f\"ðŸŽ¯ Using parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "# Prepare full dataset for training\n",
    "full_nf_data = prepare_neural_forecast_data(df, feature_categories)\n",
    "print(f\"\\nðŸ“Š Full training dataset: {len(full_nf_data)} days\")\n",
    "print(f\"ðŸ“… Training period: {full_nf_data['ds'].min()} to {full_nf_data['ds'].max()}\")\n",
    "\n",
    "# Create final model with best parameters\n",
    "final_model = NBEATSx(\n",
    "    h=HORIZON,\n",
    "    input_size=best_params['input_size'],\n",
    "    futr_exog_list=feature_categories['future_features'],\n",
    "    hist_exog_list=feature_categories['historical_features'],\n",
    "    \n",
    "    # Architecture parameters\n",
    "    stack_types=['identity', 'trend', 'seasonality'],\n",
    "    n_blocks=best_params['n_blocks'],\n",
    "    n_harmonics=best_params['n_harmonics'],\n",
    "    n_polynomials=best_params['n_polynomials'],\n",
    "    \n",
    "    # Training parameters\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_steps=best_params['max_steps'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    dropout_prob_theta=best_params.get('dropout_prob_theta', 0.0),\n",
    "    weight_decay=best_params.get('weight_decay', 1e-5),\n",
    "    early_stop_patience_steps=best_params.get('early_stop_patience_steps', 50),\n",
    "    \n",
    "    # Other settings\n",
    "    random_seed=42,\n",
    "    scaler_type='standard',\n",
    "    loss=DistributionLoss(distribution='Normal', level=[80, 90, 95]),\n",
    "    \n",
    "    # Enable progress tracking for final training\n",
    "    trainer_kwargs={\n",
    "        'enable_progress_bar': True,\n",
    "        'enable_model_summary': True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create final forecaster\n",
    "final_forecaster = NeuralForecast(models=[final_model], freq='D')\n",
    "\n",
    "# Train the final model\n",
    "print(f\"\\nðŸŽ“ Training final model...\")\n",
    "training_start = datetime.now()\n",
    "\n",
    "try:\n",
    "    final_forecaster.fit(df=full_nf_data)\n",
    "    training_success = True\n",
    "    print(\"âœ… Final model training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    training_success = False\n",
    "    print(f\"âŒ Final model training failed: {e}\")\n",
    "\n",
    "training_end = datetime.now()\n",
    "training_duration = training_end - training_start\n",
    "print(f\"â±ï¸ Training duration: {training_duration}\")\n",
    "\n",
    "if training_success:\n",
    "    print(\"\\nðŸŽ‰ Final model ready for forecasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108a8fa",
   "metadata": {},
   "source": [
    "## 5. 30-Day Balance Forecasting\n",
    "\n",
    "Generate predictions for the next 30 days with uncertainty intervals and confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb879699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 30-day forecast\n",
    "if training_success:\n",
    "    print(\"ðŸ”® Generating 30-Day Balance Forecast\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create future features for the next 30 days\n",
    "    last_date = full_nf_data['ds'].max()\n",
    "    future_features_df = create_future_features(\n",
    "        last_date=last_date,\n",
    "        horizon=HORIZON,\n",
    "        feature_categories=feature_categories\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“… Forecast period: {future_features_df['ds'].min()} to {future_features_df['ds'].max()}\")\n",
    "    \n",
    "    # Generate forecast with uncertainty intervals\n",
    "    forecast_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        forecast_df = final_forecaster.predict(futr_df=future_features_df)\n",
    "        forecasting_success = True\n",
    "        print(\"âœ… Forecast generated successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        forecasting_success = False\n",
    "        print(f\"âŒ Forecasting failed: {e}\")\n",
    "        forecast_df = None\n",
    "    \n",
    "    forecast_end = datetime.now()\n",
    "    forecast_duration = forecast_end - forecast_start\n",
    "    print(f\"â±ï¸ Forecasting duration: {forecast_duration}\")\n",
    "    \n",
    "    if forecasting_success and forecast_df is not None:\n",
    "        # Process forecast results\n",
    "        forecast_df['Date'] = future_features_df['ds']\n",
    "        forecast_df = forecast_df.reset_index(drop=True)\n",
    "        \n",
    "        # Extract predictions and confidence intervals\n",
    "        point_forecast = forecast_df['NBEATSx'].values\n",
    "        \n",
    "        # Extract confidence intervals if available\n",
    "        ci_columns = [col for col in forecast_df.columns if 'NBEATSx' in col and any(level in col for level in ['80', '90', '95'])]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Forecast Summary:\")\n",
    "        print(f\"   Horizon: {HORIZON} days\")\n",
    "        print(f\"   Point forecasts: {len(point_forecast)} values\")\n",
    "        print(f\"   Confidence intervals: {len(ci_columns)} levels\")\n",
    "        print(f\"   Available intervals: {[col.split('-')[-1] for col in ci_columns if 'hi' in col]}\")\n",
    "        \n",
    "        # Create comprehensive forecast dataframe\n",
    "        forecast_summary = pd.DataFrame({\n",
    "            'Date': future_features_df['ds'],\n",
    "            'Day': range(1, HORIZON + 1),\n",
    "            'Predicted_Balance': point_forecast\n",
    "        })\n",
    "        \n",
    "        # Add confidence intervals\n",
    "        for col in ci_columns:\n",
    "            if 'lo' in col:\n",
    "                level = col.split('-')[-1]\n",
    "                forecast_summary[f'Lower_CI_{level}'] = forecast_df[col]\n",
    "            elif 'hi' in col:\n",
    "                level = col.split('-')[-1]\n",
    "                forecast_summary[f'Upper_CI_{level}'] = forecast_df[col]\n",
    "        \n",
    "        # Add trend information\n",
    "        forecast_summary['Daily_Change'] = forecast_summary['Predicted_Balance'].diff()\n",
    "        forecast_summary['Cumulative_Change'] = forecast_summary['Predicted_Balance'] - forecast_summary['Predicted_Balance'].iloc[0]\n",
    "        forecast_summary['Weekly_Change'] = forecast_summary['Predicted_Balance'].diff(7)\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Forecast Statistics:\")\n",
    "        print(f\"   Starting balance: {point_forecast[0]:.4f}\")\n",
    "        print(f\"   Ending balance: {point_forecast[-1]:.4f}\")\n",
    "        print(f\"   Total change: {point_forecast[-1] - point_forecast[0]:.4f}\")\n",
    "        print(f\"   Average daily change: {forecast_summary['Daily_Change'].mean():.4f}\")\n",
    "        print(f\"   Max daily change: {forecast_summary['Daily_Change'].max():.4f}\")\n",
    "        print(f\"   Min daily change: {forecast_summary['Daily_Change'].min():.4f}\")\n",
    "        \n",
    "        # Display first few predictions\n",
    "        print(f\"\\nðŸ” First 10 predictions:\")\n",
    "        print(forecast_summary[['Date', 'Day', 'Predicted_Balance', 'Daily_Change']].head(10).to_string(index=False))\n",
    "        \n",
    "        # Save forecast results\n",
    "        forecast_summary.to_csv('30_day_forecast.csv', index=False)\n",
    "        forecast_summary.to_excel('30_day_forecast.xlsx', index=False)\n",
    "        print(f\"\\nðŸ’¾ Forecast saved to:\")\n",
    "        print(f\"   - 30_day_forecast.csv\")\n",
    "        print(f\"   - 30_day_forecast.xlsx\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Cannot proceed with analysis - forecasting failed\")\n",
    "        forecast_summary = None\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Cannot generate forecast - final model training failed\")\n",
    "    forecast_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast results\n",
    "if forecasting_success and forecast_summary is not None:\n",
    "    print(\"ðŸ“Š Creating Forecast Visualizations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Historical + Forecast\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Show last 60 days of historical data for context\n",
    "    hist_context = full_nf_data.tail(60)\n",
    "    ax1.plot(hist_context['ds'], hist_context['y'], 'b-', label='Historical Balance', linewidth=2)\n",
    "    ax1.plot(forecast_summary['Date'], forecast_summary['Predicted_Balance'], 'r-', \n",
    "             label='30-Day Forecast', linewidth=2, marker='o', markersize=3)\n",
    "    \n",
    "    # Add confidence intervals if available\n",
    "    if 'Lower_CI_95' in forecast_summary.columns:\n",
    "        ax1.fill_between(forecast_summary['Date'], \n",
    "                        forecast_summary['Lower_CI_95'], \n",
    "                        forecast_summary['Upper_CI_95'],\n",
    "                        alpha=0.2, color='red', label='95% Confidence')\n",
    "    \n",
    "    if 'Lower_CI_80' in forecast_summary.columns:\n",
    "        ax1.fill_between(forecast_summary['Date'], \n",
    "                        forecast_summary['Lower_CI_80'], \n",
    "                        forecast_summary['Upper_CI_80'],\n",
    "                        alpha=0.3, color='orange', label='80% Confidence')\n",
    "    \n",
    "    ax1.axvline(x=last_date, color='green', linestyle='--', alpha=0.7, label='Forecast Start')\n",
    "    ax1.set_title('Account Balance: Historical + 30-Day Forecast')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Normalized Balance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Daily Changes\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(forecast_summary['Date'], forecast_summary['Daily_Change'], 'g-', \n",
    "             marker='o', markersize=4, label='Daily Change')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.set_title('Predicted Daily Balance Changes')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Daily Change')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Cumulative Change\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(forecast_summary['Date'], forecast_summary['Cumulative_Change'], 'purple', \n",
    "             marker='o', markersize=4, label='Cumulative Change')\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax3.set_title('Cumulative Balance Change from Day 1')\n",
    "    ax3.set_xlabel('Date')\n",
    "    ax3.set_ylabel('Cumulative Change')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 4: Weekly Analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Group by week for weekly analysis\n",
    "    forecast_summary['Week'] = ((forecast_summary['Day'] - 1) // 7) + 1\n",
    "    weekly_summary = forecast_summary.groupby('Week').agg({\n",
    "        'Predicted_Balance': ['mean', 'min', 'max'],\n",
    "        'Daily_Change': 'sum'\n",
    "    }).round(4)\n",
    "    \n",
    "    weekly_summary.columns = ['Avg_Balance', 'Min_Balance', 'Max_Balance', 'Weekly_Change']\n",
    "    weekly_summary = weekly_summary.reset_index()\n",
    "    \n",
    "    ax4.bar(weekly_summary['Week'], weekly_summary['Weekly_Change'], \n",
    "            alpha=0.7, color=['green' if x >= 0 else 'red' for x in weekly_summary['Weekly_Change']])\n",
    "    ax4.set_title('Weekly Balance Changes')\n",
    "    ax4.set_xlabel('Week')\n",
    "    ax4.set_ylabel('Weekly Change')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print weekly summary\n",
    "    print(f\"\\nðŸ“Š Weekly Forecast Summary:\")\n",
    "    print(weekly_summary.to_string(index=False))\n",
    "    \n",
    "    # Save visualization\n",
    "    fig.savefig('30_day_forecast_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nðŸ’¾ Visualization saved to: 30_day_forecast_visualization.png\")\n",
    "    \n",
    "    # Risk Analysis\n",
    "    print(f\"\\nâš ï¸ RISK ANALYSIS:\")\n",
    "    \n",
    "    # Volatility analysis\n",
    "    daily_volatility = forecast_summary['Daily_Change'].std()\n",
    "    max_decline = forecast_summary['Daily_Change'].min()\n",
    "    max_increase = forecast_summary['Daily_Change'].max()\n",
    "    \n",
    "    print(f\"   ðŸ“ˆ Daily volatility (std): {daily_volatility:.4f}\")\n",
    "    print(f\"   ðŸ“‰ Largest predicted decline: {max_decline:.4f}\")\n",
    "    print(f\"   ðŸ“ˆ Largest predicted increase: {max_increase:.4f}\")\n",
    "    \n",
    "    # Trend analysis\n",
    "    if forecast_summary['Cumulative_Change'].iloc[-1] > 0:\n",
    "        trend = \"INCREASING\"\n",
    "        trend_emoji = \"ðŸ“ˆ\"\n",
    "    else:\n",
    "        trend = \"DECREASING\" \n",
    "        trend_emoji = \"ðŸ“‰\"\n",
    "    \n",
    "    print(f\"   {trend_emoji} Overall 30-day trend: {trend}\")\n",
    "    print(f\"   ðŸ’° Expected total change: {forecast_summary['Cumulative_Change'].iloc[-1]:.4f}\")\n",
    "    \n",
    "    # Weeks with negative changes\n",
    "    negative_weeks = (weekly_summary['Weekly_Change'] < 0).sum()\n",
    "    print(f\"   âš ï¸ Weeks with declining balance: {negative_weeks}/4\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Cannot create visualizations - forecasting data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d5579",
   "metadata": {},
   "source": [
    "## 6. Summary and Model Performance\n",
    "\n",
    "Final summary of the forecasting pipeline and key results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3612fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Conclusions\n",
    "print(\"ðŸŽ‰ NBEATS BALANCE FORECASTING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Project Overview\n",
    "project_summary = {\n",
    "    'model_type': 'NBEATSx Neural Network',\n",
    "    'forecast_horizon': f'{HORIZON} days',\n",
    "    'optimization_method': 'Optuna with SQLite storage',\n",
    "    'optimization_timeout': f'{TIMEOUT_HOURS} hours',\n",
    "    'data_source': 'processed_train_dataset.xlsx',\n",
    "    'output_files': [\n",
    "        '30_day_forecast.csv',\n",
    "        '30_day_forecast.xlsx', \n",
    "        '30_day_forecast_visualization.png',\n",
    "        'best_hyperparameters.json',\n",
    "        'optimization_report.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‹ PROJECT OVERVIEW:\")\n",
    "for key, value in project_summary.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Model Performance Summary\n",
    "if best_params and forecasting_success:\n",
    "    print(f\"\\nðŸ† MODEL PERFORMANCE:\")\n",
    "    print(f\"   Best validation RMSE: {study.best_value:.6f}\")\n",
    "    print(f\"   Total optimization trials: {len(study.trials)}\")\n",
    "    print(f\"   Successfully completed trials: {len(study.trials_dataframe()[study.trials_dataframe()['state'] == 'COMPLETE'])}\")\n",
    "    \n",
    "    if forecast_summary is not None:\n",
    "        print(f\"\\nðŸ“Š FORECAST CHARACTERISTICS:\")\n",
    "        print(f\"   Forecast period: {forecast_summary['Date'].min()} to {forecast_summary['Date'].max()}\")\n",
    "        print(f\"   Starting balance: {forecast_summary['Predicted_Balance'].iloc[0]:.4f}\")\n",
    "        print(f\"   Ending balance: {forecast_summary['Predicted_Balance'].iloc[-1]:.4f}\")\n",
    "        print(f\"   Total predicted change: {forecast_summary['Cumulative_Change'].iloc[-1]:.4f}\")\n",
    "        print(f\"   Average daily volatility: {forecast_summary['Daily_Change'].std():.4f}\")\n",
    "\n",
    "# Technical Details\n",
    "print(f\"\\nðŸ”§ TECHNICAL DETAILS:\")\n",
    "print(f\"   Features used: {len(feature_categories['future_features']) + len(feature_categories['historical_features'])}\")\n",
    "print(f\"   Future features: {feature_categories['future_features']}\")\n",
    "print(f\"   Historical features: {feature_categories['historical_features']}\")\n",
    "print(f\"   Training data points: {len(df)}\")\n",
    "print(f\"   Training period: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "# Execution Times\n",
    "total_execution_time = datetime.now() - start_time if 'start_time' in locals() else \"Not available\"\n",
    "print(f\"\\nâ±ï¸ EXECUTION TIMES:\")\n",
    "print(f\"   Total execution: {total_execution_time}\")\n",
    "if 'duration' in locals():\n",
    "    print(f\"   Optimization duration: {duration}\")\n",
    "if 'training_duration' in locals():\n",
    "    print(f\"   Final training duration: {training_duration}\")\n",
    "if 'forecast_duration' in locals():\n",
    "    print(f\"   Forecasting duration: {forecast_duration}\")\n",
    "\n",
    "# Files Generated\n",
    "print(f\"\\nðŸ’¾ FILES GENERATED:\")\n",
    "import os\n",
    "output_files = [\n",
    "    '30_day_forecast.csv',\n",
    "    '30_day_forecast.xlsx',\n",
    "    '30_day_forecast_visualization.png',\n",
    "    'best_hyperparameters.json',\n",
    "    'optimization_report.json',\n",
    "    f'optuna_study_{STUDY_NAME}.db'\n",
    "]\n",
    "\n",
    "for file in output_files:\n",
    "    if os.path.exists(file):\n",
    "        file_size = os.path.getsize(file)\n",
    "        print(f\"   âœ… {file} ({file_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"   âŒ {file} (not found)\")\n",
    "\n",
    "# Next Steps and Recommendations\n",
    "print(f\"\\nðŸš€ NEXT STEPS & RECOMMENDATIONS:\")\n",
    "print(\"   1. Review forecast visualization for business insights\")\n",
    "print(\"   2. Monitor actual vs predicted values to validate model performance\")\n",
    "print(\"   3. Update model weekly/monthly with new data\")\n",
    "print(\"   4. Consider ensemble methods for improved accuracy\")\n",
    "print(\"   5. Implement automated retraining pipeline\")\n",
    "print(\"   6. Set up monitoring alerts for significant forecast deviations\")\n",
    "\n",
    "# Success Status\n",
    "overall_success = (\n",
    "    best_params is not None and \n",
    "    training_success and \n",
    "    forecasting_success and \n",
    "    forecast_summary is not None\n",
    ")\n",
    "\n",
    "print(f\"\\n{'ðŸŽ‰' if overall_success else 'âš ï¸'} OVERALL STATUS: {'SUCCESS' if overall_success else 'PARTIAL SUCCESS'}\")\n",
    "\n",
    "if overall_success:\n",
    "    print(\"âœ… All pipeline components completed successfully!\")\n",
    "    print(\"âœ… 30-day balance forecast is ready for business use!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some components may need attention - check logs above\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ NBEATS BALANCE FORECASTING COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
